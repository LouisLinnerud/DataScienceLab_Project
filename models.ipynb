{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "Trying out different models and paramaters to see which performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(87811) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the evaluation data\n",
    "df = pd.read_csv(\"../csv_files/development.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the positions, removes the x and y column and splitting the data into train and validation set\n",
    "def extraxting_positions(df):\n",
    "    pos = []\n",
    "    for i in range(len(df)):\n",
    "        pos.append([df[\"x\"].iloc[i], df[\"y\"].iloc[i]])\n",
    "    return pos\n",
    "\n",
    "pos = extraxting_positions(df)\n",
    "\n",
    "## Dropping data from x and y \n",
    "df.drop([\"x\", \"y\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing pads with format: pads = [\"0\", \"7\", \"12\", ..]\n",
    "def drop_pads(df, pads):\n",
    "    cols_to_drop = [col for col in df.columns if any(idx in col for idx in pads)]\n",
    "    df_removed = df.drop(cols_to_drop, axis=1)    \n",
    "    return df_removed\n",
    "\n",
    "remove_pads = [\"0\", \"7\", \"12\", \"15\", \"16\", \"17\"]\n",
    "df_removed_noise = drop_pads(df, remove_pads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting into train and validation set\n",
    "X_train, X_val, pos_train, pos_val = train_test_split(df_removed_noise, pos, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn multioutputregressor \n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "mult_regr = MultiOutputRegressor(LinearRegression())\n",
    "mult_regr.fit(X_train, pos_train)\n",
    "pos_pred = mult_regr.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error = 12.32\n",
      "Mean squared error = 300.38\n",
      "Median absolute error = 8.82\n",
      "Explain variance score = 0.98\n",
      "R2 score = 0.98\n"
     ]
    }
   ],
   "source": [
    "# Metrics to evaluating model \n",
    "import sklearn.metrics as sm\n",
    "\n",
    "def metrics_on_model(pos_val, pos_pred):\n",
    "    print(\"Mean absolute error =\", round(sm.mean_absolute_error(pos_val, pos_pred), 2)) \n",
    "    print(\"Mean squared error =\", round(sm.mean_squared_error(pos_val, pos_pred), 2)) \n",
    "    print(\"Median absolute error =\", round(sm.median_absolute_error(pos_val, pos_pred), 2)) \n",
    "    print(\"Explain variance score =\", round(sm.explained_variance_score(pos_val, pos_pred), 2)) \n",
    "    print(\"R2 score =\", round(sm.r2_score(pos_val, pos_pred), 2))\n",
    "    #print(\"Mean eucledian distance =\", round(sm.euclidean_distances(pos_val, pos_pred), 2))\n",
    "\n",
    "metrics_on_model(pos_val, pos_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the multiple_reg_model on the evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_data = pd.read_csv(\"../csv_files/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the ID\n",
    "eval_id = ev_data[\"Id\"]\n",
    "\n",
    "# Dropping the Id column from the ev_data\n",
    "ev_data.drop([\"Id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the position array to a string to be used in the .csv file \n",
    "def pred_to_string(prediction_array):\n",
    "    pred_column = []\n",
    "    for i in range(len(prediction_array)):\n",
    "        pos_string = (str(prediction_array[i][0]) + \"|\" + str(prediction_array[i][1]))\n",
    "        pred_column.append(pos_string)\n",
    "    return pred_column\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the evaluation results\n",
    "mult_regr_eval = mult_regr.predict(ev_data)\n",
    "pos_pred = pred_to_string(mult_regr_eval) # Formatting the predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a df and .csv file to be submitted. Saved in submission_file folder\n",
    "mult_reg_submission = pd.DataFrame({'Id': eval_id, 'Predicted': pos_pred})\n",
    "mult_reg_submission.to_csv(\"../DataScienceLab_Project/submission_files/mult_reg_first_try.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
