{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "Trying out different models and paramaters to see which performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the evaluation data\n",
    "dev = pd.read_csv(\"../csv_files/development.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sum all areas to one col\n",
    "def sum_area(dframe):\n",
    "    area_columns = [col for col in dframe.columns if col.startswith('area')]\n",
    "    sum_of_areas = dframe[area_columns].sum(axis=1)\n",
    "    dframe['area_sum'] = sum_of_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the positions and removes the x and y column.\n",
    "import numpy as np\n",
    "pos_dev = dev[[\"x\", \"y\"]]\n",
    "\n",
    "## Dropping data from x and y \n",
    "dev = dev.drop([\"x\", \"y\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing pads with format: pads = [0, 7, 12, ..]\n",
    "def drop_pads(input_list, df):\n",
    "    for i in input_list:\n",
    "        columns_to_remove = df.filter(like=f'[{i}]').columns\n",
    "        df = df.drop(columns=columns_to_remove)\n",
    "    return df\n",
    "\n",
    "remove_pads = [0, 7, 12, 15, 16, 17]\n",
    "dev_removed_noise = drop_pads(remove_pads, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing rms feature\n",
    "def drop_rms_features(df):\n",
    "    # Extract columns that start with 'rms'\n",
    "    rms_columns = [col for col in df.columns if not col.startswith('rms')]\n",
    "\n",
    "    # Create a new DataFrame without 'rms' columns\n",
    "    df_without_rms = df[rms_columns] \n",
    "    return df_without_rms\n",
    "\n",
    "dev_interesting_data = drop_rms_features(dev_removed_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing tmax feature\n",
    "def drop_tmax_features(df):\n",
    "    # Extract columns that start with 'rms'\n",
    "    tmax_columns = [col for col in df.columns if not col.startswith('tmax')]\n",
    "\n",
    "    # Create a new DataFrame without 'rms' columns\n",
    "    df_without_tmax = df[tmax_columns] \n",
    "    return df_without_tmax\n",
    "\n",
    "dev_interesting_data = drop_tmax_features(dev_interesting_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Z-transformation of the data. Remember to scale accordingly to training data for eval data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dev_interesting_data)\n",
    " \n",
    "dev_interesting_data = pd.DataFrame(scaler.transform(dev_interesting_data), columns=dev_interesting_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile2(dframe, lw=0.05, up=0.95, drop=True):\n",
    "    tresholds = {}\n",
    "    for col_name in dframe.columns:\n",
    "        lw_tresh = dframe[col_name].quantile(lw)\n",
    "        up_tresh = dframe[col_name].quantile(up)\n",
    "        tresholds[col_name] = [lw_tresh, up_tresh]\n",
    "    print(f\"tresholds for {lw}, {up}: {tresholds}\")\n",
    "    initial_dim = dframe.shape\n",
    "    for col_name in dframe.columns:\n",
    "        if drop:\n",
    "            dframe.drop(dframe[dframe[col_name] < tresholds[col_name][0]].index, inplace=True)\n",
    "            dframe.drop(dframe[dframe[col_name] > tresholds[col_name][1]].index, inplace=True)\n",
    "        else:\n",
    "            dframe.loc[dframe[col_name] < tresholds[col_name][0], col_name] = tresholds[col_name][0]\n",
    "            dframe.loc[dframe[col_name] > tresholds[col_name][1], col_name] = tresholds[col_name][1]\n",
    "\n",
    "    new_dim = dframe.shape\n",
    "    print(f\"\"\"\n",
    "          initial dim:   {initial_dim}\n",
    "          new dim:       {new_dim}\n",
    "          a reduction of {((initial_dim[0]-new_dim[0])/initial_dim[0])*100}% of rows\n",
    "          \"\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Reducing the dataset to X percent of original size to speed up model testing\n",
    "# dev_interesting_data_sample = dev_interesting_data.sample(frac=0.25)\n",
    "#pos_dev_sample = pos_dev.loc[dev_interesting_data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting into train and validation set\n",
    "X_train, X_val, pos_train, pos_val = train_test_split(dev_interesting_data, pos_dev, test_size=0.001, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tresholds for 0.01, 0.99: {'pmax[1]': [3.438953704833962, 78.60699914550783], 'negpmax[1]': [-46.69329519653318, -2.974553710937477], 'area[1]': [1.5165100158690865, 39.688086157226614], 'pmax[2]': [3.191000152587868, 44.261093963623104], 'negpmax[2]': [-23.78017808261275, -2.947936126708984], 'area[2]': [1.2261787353515805, 24.626779455566098], 'pmax[3]': [3.9980726928710486, 105.3254884033204], 'negpmax[3]': [-63.156826782226545, -3.0886613023651734], 'area[3]': [2.1880904113770163, 51.3920994689941], 'pmax[4]': [3.4978090026881783, 63.28823794555664], 'negpmax[4]': [-35.70557189941402, -3.0864365650258394], 'area[4]': [1.5376550354004723, 32.69773432617193], 'pmax[5]': [6.581597076416038, 111.8521913757324], 'negpmax[5]': [-65.75915573120119, -3.5292998046875446], 'area[5]': [4.538289691162173, 54.45958193359374], 'pmax[6]': [3.430690277099609, 84.86641152954105], 'negpmax[6]': [-53.70983749389651, -3.0848340213749554], 'area[6]': [1.46019553833006, 42.110686480712936], 'pmax[8]': [3.5027126159667525, 91.06780700683599], 'negpmax[8]': [-58.093145843505816, -3.269603240966751], 'area[8]': [1.5721669616698997, 44.44288839111339], 'pmax[9]': [3.229211334228538, 72.92005838012712], 'negpmax[9]': [-45.53949987792971, -3.113927441599416], 'area[9]': [1.2984619018555141, 35.96684974365241], 'pmax[10]': [5.488835723876931, 116.51789642333982], 'negpmax[10]': [-71.81322949218752, -3.5591907084249788], 'area[10]': [3.5608904174804152, 56.16738402099626], 'pmax[11]': [3.7894417381846432, 96.36578485107428], 'negpmax[11]': [-59.94929412841801, -3.3363944142945225], 'area[11]': [1.782954791259811, 47.09511898193362], 'pmax[13]': [7.800182983398437, 113.7841207275391], 'negpmax[13]': [-69.75073510742187, -3.8142312225561383], 'area[13]': [4.947834381103485, 55.43956562499995], 'pmax[14]': [3.500972338820272, 84.98440606689455], 'negpmax[14]': [-52.920958740234354, -3.3307717681480526], 'area[14]': [1.4681418823242638, 42.09666834106439], 'area_sum': [1389.2943593688958, 1552.0932416015637]}\n",
      "\n",
      "          initial dim:   (308400, 37)\n",
      "          new dim:       (308400, 37)\n",
      "          a reduction of 0.0% of rows\n",
      "          \n"
     ]
    }
   ],
   "source": [
    "## drop the outliers\n",
    "quantile2(X_train, 0.01, 0.99,drop=False)\n",
    "pos_train = pos_train.loc[X_train.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.4s\n"
     ]
    }
   ],
   "source": [
    "# RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import math\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "numb_trees = 50\n",
    "base_regressor = RandomForestRegressor(n_estimators=numb_trees, criterion=\"poisson\", max_depth=30, max_features=0.3, bootstrap=True, verbose=1) \n",
    "mult_regr = MultiOutputRegressor(base_regressor)\n",
    "\n",
    "mult_regr.fit(X_train, pos_train)\n",
    "\n",
    "pos_pred = mult_regr.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on GridSearchCV\n",
    "By splitting the problem into a regression problem for x-coordinate and y-coordinate I used GridSearchCV with these paramaters:\n",
    "   \n",
    "    param_grid = {  \n",
    "        'max_depth': [20, 30, 40],  # Maximum depth of the tree\n",
    "        'min_samples_split':[2, 4, 6], # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2, 4], # Minimum number of samples required to be at a leaf node\n",
    "        'max_features': [\"sqrt\", \"log2\", None]\n",
    "        }  \n",
    "         \n",
    "The best paramaters for x-coordinate and y-coordinate were: \n",
    "  \n",
    "{'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}  \n",
    "{'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 4}  \n",
    "\n",
    " \n",
    " This GridSearch is run on only 1% of the dataset in order to speed up the process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error = 2.7\n",
      "Mean squared error = 12.96\n",
      "Median absolute error = 2.19\n",
      "Explain variance score = 1.0\n",
      "R2 score = 1.0\n",
      "Mean eucledian distance = 4.27\n"
     ]
    }
   ],
   "source": [
    "# Metrics to evaluating model \n",
    "import sklearn.metrics as sm\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def avg_euc_dist(pos_val, pos_pred):\n",
    "    sum_square = 0\n",
    "    for i in range(len(pos_val)):\n",
    "        sum_square += math.sqrt((pos_val[i][0]-pos_pred[i][0])**2 + (pos_val[i][1]-pos_pred[i][1])**2)\n",
    "    return sum_square/len(pos_val)    \n",
    "\n",
    "def metrics_on_model(pos_val, pos_pred):\n",
    "    print(\"Mean absolute error =\", round(sm.mean_absolute_error(pos_val, pos_pred), 2)) \n",
    "    print(\"Mean squared error =\", round(sm.mean_squared_error(pos_val, pos_pred), 2)) \n",
    "    print(\"Median absolute error =\", round(sm.median_absolute_error(pos_val, pos_pred), 2)) \n",
    "    print(\"Explain variance score =\", round(sm.explained_variance_score(pos_val, pos_pred), 2)) \n",
    "    print(\"R2 score =\", round(sm.r2_score(pos_val, pos_pred), 2))\n",
    "    print(\"Mean eucledian distance =\", round(avg_euc_dist(pos_val, pos_pred), 2))\n",
    "\n",
    "metrics_on_model(pos_val.to_numpy(), pos_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "\n",
    "Test 1:  \n",
    "    Data: reomving pads (0, 7, 12, 15, 16, 17)  \n",
    "    Number of trees: 10   \n",
    "    Mean euc dist: 5.77  \n",
    "\n",
    "Test 2:  \n",
    "    Data: reomving pads (0, 7, 12, 15, 16, 17) and removing rms feature  \n",
    "    Number of trees: 10  \n",
    "    Mean euc dist: 5.69\n",
    "\n",
    "Test 3:    \n",
    "    Data: Sampeled 25% of the data to speed up model training. Reomving pads (0, 7, 12, 15, 16, 17) and removing rms feature.  \n",
    "    Number of trees: 10  \n",
    "    Mean euc dist: 6.56  \n",
    "\n",
    "Test 4:    \n",
    "    Data: Sampeled 25% of the data to speed up model training. Reomving pads (0, 7, 12, 15, 16, 17) and removing rms feature.  \n",
    "    Number of trees: 30  \n",
    "    Mean euc dist: 6.16  \n",
    "\n",
    "Test 4:    \n",
    "    Data: With PCA. Sampeled 25% of the data to speed up model training. Reomving pads (0, 7, 12, 15, 16, 17) and removing rms feature.  \n",
    "    Number of trees: 30  \n",
    "    Mean euc dist: 10.29\n",
    "\n",
    "Test 5:        \n",
    "    Data: Reomving pads (0, 7, 12, 15, 16, 17) and removing rms feature. Added RobustScaling.   \n",
    "    Number of trees: 50  \n",
    "    Mean euc dist: 5.31  \n",
    "\n",
    "Test 6:  \n",
    "    Data: Reomving pads (0, 7, 12, 15, 16, 17) and removing rms feature. Added column: pmax[5]*area[5] \n",
    "    Number of trees: 30  \n",
    "    Mean euc dist: 6.11\n",
    "\n",
    "Test 7:   \n",
    "    Data: Reomving pads (0, 7, 12, 15, 16, 17) and removing rms feature.  \n",
    "    Number of trees: 25  \n",
    "    Mean euc dist: 6.58  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the multiple_reg_model on the evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_data = pd.read_csv(\"../csv_files/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the ID\n",
    "eval_id = ev_data[\"Id\"]\n",
    "\n",
    "# Dropping the Id column from the ev_data\n",
    "ev_data = ev_data.drop([\"Id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the position array to a string to be used in the .csv file \n",
    "def pred_to_string(prediction_array):\n",
    "    pred_column = []\n",
    "    for i in range(len(prediction_array)):\n",
    "        pos_string = (str(prediction_array[i][0]) + \"|\" + str(prediction_array[i][1]))\n",
    "        pred_column.append(pos_string)\n",
    "    return pred_column\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing \n",
    "remove_pads = [0, 7, 12, 15, 16, 17]\n",
    "ev_data = drop_pads(remove_pads, ev_data) # Remove pads\n",
    "ev_data = drop_rms_features(ev_data) # Remove rms feature \n",
    "ev_data = drop_tmax_features(ev_data)\n",
    "#sum_area(ev_data)\n",
    "#ev_data = pd.DataFrame(scaler.transform(ev_data), columns=ev_data.columns) # Z-transform with mean and std from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    4.2s\n"
     ]
    }
   ],
   "source": [
    "# Predicting the evaluation results\n",
    "mult_regr_eval = mult_regr.predict(ev_data)\n",
    "pos_pred = pred_to_string(mult_regr_eval) # Formatting the predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a df and .csv file to be submitted. Saved in submission_file folder\n",
    "mult_reg_submission = pd.DataFrame({'Id': eval_id, 'Predicted': pos_pred})\n",
    "mult_reg_submission.to_csv(\"../DataScienceLab_Project/submission_files/vanilla_all_n50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
