{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "Trying out different models and paramaters to see which performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the evaluation data\n",
    "dev = pd.read_csv(\"../csv_files/development.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the positions and removes the x and y column.\n",
    "import numpy as np\n",
    "pos_dev = dev[[\"x\", \"y\"]]\n",
    "\n",
    "## Dropping data from x and y \n",
    "dev = dev.drop([\"x\", \"y\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing pads with format: pads = [0, 7, 12, ..]\n",
    "def drop_pads(input_list, df):\n",
    "    for i in input_list:\n",
    "        columns_to_remove = df.filter(like=f'[{i}]').columns\n",
    "        df = df.drop(columns=columns_to_remove)\n",
    "    return df\n",
    "\n",
    "remove_pads = [0, 7, 12, 15, 16, 17]\n",
    "dev_removed_noise = drop_pads(remove_pads, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing rms feature\n",
    "def drop_rms_features(df):\n",
    "    # Extract columns that start with 'rms'\n",
    "    rms_columns = [col for col in df.columns if not col.startswith('rms')]\n",
    "\n",
    "    # Create a new DataFrame without 'rms' columns\n",
    "    df_without_rms = df[rms_columns] \n",
    "    return df_without_rms\n",
    "\n",
    "dev_interesting_data = drop_rms_features(dev_removed_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Z-transformation of the data. Remember to scale accordingly to training data for eval data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dev_interesting_data)\n",
    " \n",
    "dev_interesting_data = pd.DataFrame(scaler.transform(dev_interesting_data), columns=dev_interesting_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tresholds for 0.05, 0.95: {'pmax[1]': [-0.7407320546539865, 2.3402594317666328], 'negpmax[1]': [-0.7208474392297578, 0.18945015391859168], 'area[1]': [-0.9724167655602981, 2.2226618304555585], 'tmax[1]': [-1.2186556275587128, 2.611600302360536], 'pmax[2]': [-0.6556721471489008, 1.9669523659501227], 'negpmax[2]': [-0.046347206330420034, 0.01233168747810533], 'area[2]': [-0.9432993236621513, 1.9061335046685997], 'tmax[2]': [-1.4896704626382784, 2.237840076127894], 'pmax[3]': [-0.8403407634289222, 2.2843787882130853], 'negpmax[3]': [-0.5197525858018354, 0.1505719076960286], 'area[3]': [-0.9636576770136466, 2.228705291077654], 'tmax[3]': [-0.161792632723619, -0.025560027867424884], 'pmax[4]': [-0.8053137402772753, 2.3405959442671875], 'negpmax[4]': [-0.13456623860900593, 0.041836061375541356], 'area[4]': [-1.0398481818718164, 2.2121038015302315], 'tmax[4]': [-1.1684536912547046, 2.6237900342584273], 'pmax[5]': [-1.0633164912918998, 1.9661748825877787], 'negpmax[5]': [-2.0073135205653077, 0.9199969386964814], 'area[5]': [-1.1508095698348717, 1.93228514581882], 'tmax[5]': [-0.23550460310166657, 0.2135493623191846], 'pmax[6]': [-0.6528478248945169, 2.336181624280497], 'negpmax[6]': [-0.8338116637096498, 0.1980750992971113], 'area[6]': [-0.8635193758806569, 2.2631223840641286], 'tmax[6]': [-1.2968208311204856, 2.602476854735463], 'pmax[8]': [-0.80653148055468, 2.2675613014424054], 'negpmax[8]': [-0.1514362576621072, 0.045335516256326966], 'area[8]': [-0.9781217734928264, 2.210624744314782], 'tmax[8]': [-0.7657282007081282, 2.327728491278854], 'pmax[9]': [-0.6277318418841, 2.3176381619968853], 'negpmax[9]': [-0.2301629880715485, 0.0620761173780653], 'area[9]': [-0.856658484815229, 2.2247954199234377], 'tmax[9]': [-1.4658403402038829, 2.5082478180398007], 'pmax[10]': [-0.9916946762086508, 2.006572673757597], 'negpmax[10]': [-0.31278042550591834, 0.13600169392985778], 'area[10]': [-1.0702769686367004, 1.9717511558829925], 'tmax[10]': [-0.18363770942328084, 0.09447621628370464], 'pmax[11]': [-0.8090902626518428, 2.2740099138905197], 'negpmax[11]': [-0.2954573227808946, 0.08525852215750195], 'area[11]': [-0.9605627129772635, 2.2083926116702064], 'tmax[11]': [-0.18662095772321538, 0.9572471511540909], 'pmax[13]': [-1.1071527422240153, 1.9672226268478552], 'negpmax[13]': [-1.0222527302143685, 0.496596764611861], 'area[13]': [-1.1929107583403435, 1.9418160710205281], 'tmax[13]': [-0.3067486865163614, 0.3003396141252003], 'pmax[14]': [-0.7024811602431431, 2.473130937509652], 'negpmax[14]': [-0.17043908417495096, 0.039114213235327364], 'area[14]': [-0.8863258827533768, 2.3953827176332405], 'tmax[14]': [-1.2106182285506746, 2.5710322889341937]}\n",
      "\n",
      "          initial dim:   (385500, 48)\n",
      "          new dim:       (385500, 48)\n",
      "          a reduction of 0.0% of rows\n",
      "          \n"
     ]
    }
   ],
   "source": [
    "def quantile2(dframe, lw=0.05, up=0.95, drop=True):\n",
    "    tresholds = {}\n",
    "    for col_name in dframe.columns:\n",
    "        lw_tresh = dframe[col_name].quantile(lw)\n",
    "        up_tresh = dframe[col_name].quantile(up)\n",
    "        tresholds[col_name] = [lw_tresh, up_tresh]\n",
    "    print(f\"tresholds for {lw}, {up}: {tresholds}\")\n",
    "    initial_dim = dframe.shape\n",
    "    for col_name in dframe.columns:\n",
    "        if drop:\n",
    "            dframe.drop(dframe[dframe[col_name] < tresholds[col_name][0]].index, inplace=True)\n",
    "            dframe.drop(dframe[dframe[col_name] > tresholds[col_name][1]].index, inplace=True)\n",
    "        else:\n",
    "            dframe.loc[dframe[col_name] < tresholds[col_name][0], col_name] = tresholds[col_name][0]\n",
    "            dframe.loc[dframe[col_name] > tresholds[col_name][1], col_name] = tresholds[col_name][1]\n",
    "\n",
    "    new_dim = dframe.shape\n",
    "    print(f\"\"\"\n",
    "          initial dim:   {initial_dim}\n",
    "          new dim:       {new_dim}\n",
    "          a reduction of {((initial_dim[0]-new_dim[0])/initial_dim[0])*100}% of rows\n",
    "          \"\"\")\n",
    "    \n",
    "quantile2(dev_interesting_data, drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Reducing the dataset to X percent of original size to speed up model testing\n",
    "# dev_interesting_data_sample = dev_interesting_data.sample(frac=0.25)\n",
    "pos_dev_sample = pos_dev.loc[dev_interesting_data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting into train and validation set\n",
    "X_train, X_val, pos_train, pos_val = train_test_split(dev_interesting_data, pos_dev_sample, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import math\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "numb_trees = 35\n",
    "base_regressor = RandomForestRegressor(n_estimators=numb_trees, criterion=\"poisson\", max_depth=30, max_features=0.3, bootstrap=True) \n",
    "mult_regr = MultiOutputRegressor(base_regressor)\n",
    "\n",
    "mult_regr.fit(X_train, pos_train)\n",
    "\n",
    "pos_pred = mult_regr.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on GridSearchCV\n",
    "By splitting the problem into a regression problem for x-coordinate and y-coordinate I used GridSearchCV with these paramaters:\n",
    "   \n",
    "    param_grid = {  \n",
    "        'max_depth': [20, 30, 40],  # Maximum depth of the tree\n",
    "        'min_samples_split':[2, 4, 6], # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2, 4], # Minimum number of samples required to be at a leaf node\n",
    "        'max_features': [\"sqrt\", \"log2\", None]\n",
    "        }  \n",
    "         \n",
    "The best paramaters for x-coordinate and y-coordinate were: \n",
    "  \n",
    "{'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}  \n",
    "{'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 4}  \n",
    "\n",
    " \n",
    " This GridSearch is run on only 1% of the dataset in order to speed up the process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error = 2.81\n",
      "Mean squared error = 14.13\n",
      "Median absolute error = 2.29\n",
      "Explain variance score = 1.0\n",
      "R2 score = 1.0\n",
      "Mean eucledian distance = 4.43\n"
     ]
    }
   ],
   "source": [
    "# Metrics to evaluating model \n",
    "import sklearn.metrics as sm\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def avg_euc_dist(pos_val, pos_pred):\n",
    "    sum_square = 0\n",
    "    for i in range(len(pos_val)):\n",
    "        sum_square += math.sqrt((pos_val[i][0]-pos_pred[i][0])**2 + (pos_val[i][1]-pos_pred[i][1])**2)\n",
    "    return sum_square/len(pos_val)    \n",
    "\n",
    "def metrics_on_model(pos_val, pos_pred):\n",
    "    print(\"Mean absolute error =\", round(sm.mean_absolute_error(pos_val, pos_pred), 2)) \n",
    "    print(\"Mean squared error =\", round(sm.mean_squared_error(pos_val, pos_pred), 2)) \n",
    "    print(\"Median absolute error =\", round(sm.median_absolute_error(pos_val, pos_pred), 2)) \n",
    "    print(\"Explain variance score =\", round(sm.explained_variance_score(pos_val, pos_pred), 2)) \n",
    "    print(\"R2 score =\", round(sm.r2_score(pos_val, pos_pred), 2))\n",
    "    print(\"Mean eucledian distance =\", round(avg_euc_dist(pos_val, pos_pred), 2))\n",
    "\n",
    "metrics_on_model(pos_val.to_numpy(), pos_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "\n",
    "Test 1:  \n",
    "    Data: reomving pads (0, 7, 12, 15, 16, 17)  \n",
    "    Number of trees: 10   \n",
    "    Mean euc dist: 5.77  \n",
    "\n",
    "Test 2:  \n",
    "    Data: reomving pads (0, 7, 12, 15, 16, 17) and removing rms feature  \n",
    "    Number of trees: 10  \n",
    "    Mean euc dist: 5.69\n",
    "\n",
    "Test 3:    \n",
    "    Data: Sampeled 25% of the data to speed up model training. Reomving pads (0, 7, 12, 15, 16, 17) and removing rms feature.  \n",
    "    Number of trees: 10  \n",
    "    Mean euc dist: 6.56  \n",
    "\n",
    "Test 4:    \n",
    "    Data: Sampeled 25% of the data to speed up model training. Reomving pads (0, 7, 12, 15, 16, 17) and removing rms feature.  \n",
    "    Number of trees: 30  \n",
    "    Mean euc dist: 6.16  \n",
    "\n",
    "Test 4:    \n",
    "    Data: With PCA. Sampeled 25% of the data to speed up model training. Reomving pads (0, 7, 12, 15, 16, 17) and removing rms feature.  \n",
    "    Number of trees: 30  \n",
    "    Mean euc dist: 10.29\n",
    "\n",
    "Test 5:        \n",
    "    Data: Reomving pads (0, 7, 12, 15, 16, 17) and removing rms feature. Added RobustScaling.   \n",
    "    Number of trees: 50  \n",
    "    Mean euc dist: 5.31  \n",
    "\n",
    "Test 6:  \n",
    "    Data: Reomving pads (0, 7, 12, 15, 16, 17) and removing rms feature. Added column: pmax[5]*area[5] \n",
    "    Number of trees: 30  \n",
    "    Mean euc dist: 6.11\n",
    "\n",
    "Test 7:   \n",
    "    Data: Reomving pads (0, 7, 12, 15, 16, 17) and removing rms feature.  \n",
    "    Number of trees: 25  \n",
    "    Mean euc dist: 6.58  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the multiple_reg_model on the evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_data = pd.read_csv(\"../csv_files/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the ID\n",
    "eval_id = ev_data[\"Id\"]\n",
    "\n",
    "# Dropping the Id column from the ev_data\n",
    "ev_data = ev_data.drop([\"Id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the position array to a string to be used in the .csv file \n",
    "def pred_to_string(prediction_array):\n",
    "    pred_column = []\n",
    "    for i in range(len(prediction_array)):\n",
    "        pos_string = (str(prediction_array[i][0]) + \"|\" + str(prediction_array[i][1]))\n",
    "        pred_column.append(pos_string)\n",
    "    return pred_column\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing \n",
    "remove_pads = [0, 7, 12, 15, 16, 17]\n",
    "ev_data = drop_pads(remove_pads, ev_data) # Remove pads\n",
    "ev_data = drop_rms_features(ev_data) # Remove rms feature \n",
    "ev_data = pd.DataFrame(scaler.transform(ev_data), columns=ev_data.columns) # Z-transform with mean and std from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- area[0]\n- area[10]\n- area[12]\n- area[15]\n- area[16]\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Predicting the evaluation results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mult_regr_eval \u001b[39m=\u001b[39m mult_regr\u001b[39m.\u001b[39;49mpredict(ev_data)\n\u001b[1;32m      3\u001b[0m pos_pred \u001b[39m=\u001b[39m pred_to_string(mult_regr_eval) \u001b[39m# Formatting the predictions \u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/skole/4.klasse/7.semester/Data science lab/semesteroppgave/DSLvenv/lib/python3.11/site-packages/sklearn/multioutput.py:305\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_[\u001b[39m0\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    303\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe base estimator should implement a predict method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 305\u001b[0m y \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[1;32m    306\u001b[0m     delayed(e\u001b[39m.\u001b[39;49mpredict)(X) \u001b[39mfor\u001b[39;49;00m e \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimators_\n\u001b[1;32m    307\u001b[0m )\n\u001b[1;32m    309\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(y)\u001b[39m.\u001b[39mT\n",
      "File \u001b[0;32m~/Documents/skole/4.klasse/7.semester/Data science lab/semesteroppgave/DSLvenv/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/Documents/skole/4.klasse/7.semester/Data science lab/semesteroppgave/DSLvenv/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Documents/skole/4.klasse/7.semester/Data science lab/semesteroppgave/DSLvenv/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Documents/skole/4.klasse/7.semester/Data science lab/semesteroppgave/DSLvenv/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/skole/4.klasse/7.semester/Data science lab/semesteroppgave/DSLvenv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:984\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    982\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m    983\u001b[0m \u001b[39m# Check data\u001b[39;00m\n\u001b[0;32m--> 984\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_X_predict(X)\n\u001b[1;32m    986\u001b[0m \u001b[39m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[1;32m    987\u001b[0m n_jobs, _, _ \u001b[39m=\u001b[39m _partition_estimators(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n",
      "File \u001b[0;32m~/Documents/skole/4.klasse/7.semester/Data science lab/semesteroppgave/DSLvenv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:599\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[39mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[1;32m    598\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 599\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, dtype\u001b[39m=\u001b[39;49mDTYPE, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    600\u001b[0m \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m (X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc):\n\u001b[1;32m    601\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/skole/4.klasse/7.semester/Data science lab/semesteroppgave/DSLvenv/lib/python3.11/site-packages/sklearn/base.py:580\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_data\u001b[39m(\n\u001b[1;32m    510\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    511\u001b[0m     X\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno_validation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params,\n\u001b[1;32m    517\u001b[0m ):\n\u001b[1;32m    518\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \n\u001b[1;32m    520\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[39m        validated.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_feature_names(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[1;32m    582\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    583\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    584\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m estimator \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    585\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequires y to be passed, but the target y is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/skole/4.klasse/7.semester/Data science lab/semesteroppgave/DSLvenv/lib/python3.11/site-packages/sklearn/base.py:507\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m missing_names \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    503\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    504\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n\u001b[0;32m--> 507\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- area[0]\n- area[10]\n- area[12]\n- area[15]\n- area[16]\n- ...\n"
     ]
    }
   ],
   "source": [
    "# Predicting the evaluation results\n",
    "mult_regr_eval = mult_regr.predict(ev_data)\n",
    "pos_pred = pred_to_string(mult_regr_eval) # Formatting the predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Per-column arrays must each be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Creating a df and .csv file to be submitted. Saved in submission_file folder\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mult_reg_submission \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m'\u001b[39;49m\u001b[39mId\u001b[39;49m\u001b[39m'\u001b[39;49m: eval_id, \u001b[39m'\u001b[39;49m\u001b[39mPredicted\u001b[39;49m\u001b[39m'\u001b[39;49m: pos_pred})\n\u001b[1;32m      3\u001b[0m mult_reg_submission\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39m../DataScienceLab_Project/submission_files/mult_reg_first_try.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/skole/4.klasse/7.semester/Data science lab/semesteroppgave/DSLvenv/lib/python3.11/site-packages/pandas/core/frame.py:733\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    727\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    728\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    729\u001b[0m     )\n\u001b[1;32m    731\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    732\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    734\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    735\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/Documents/skole/4.klasse/7.semester/Data science lab/semesteroppgave/DSLvenv/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/Documents/skole/4.klasse/7.semester/Data science lab/semesteroppgave/DSLvenv/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/Documents/skole/4.klasse/7.semester/Data science lab/semesteroppgave/DSLvenv/lib/python3.11/site-packages/pandas/core/internals/construction.py:664\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    662\u001b[0m         raw_lengths\u001b[39m.\u001b[39mappend(\u001b[39mlen\u001b[39m(val))\n\u001b[1;32m    663\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(val, np\u001b[39m.\u001b[39mndarray) \u001b[39mand\u001b[39;00m val\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 664\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m indexes \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m    667\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf using all scalar values, you must pass an index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Per-column arrays must each be 1-dimensional"
     ]
    }
   ],
   "source": [
    "# Creating a df and .csv file to be submitted. Saved in submission_file folder\n",
    "mult_reg_submission = pd.DataFrame({'Id': eval_id, 'Predicted': pos_pred})\n",
    "mult_reg_submission.to_csv(\"../DataScienceLab_Project/submission_files/mult_reg_rand_forest_tuned_hyperparam_z_score_correct_pads.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
